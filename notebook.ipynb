{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cmjOVVtJdiPZ"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/cg123/ties-merge.git\n",
        "%cd /content/ties-merge\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_MODEL = \"TheBloke/Llama-2-13B-fp16\"\n",
        "MERGE_MODELS = [\n",
        "    \"WizardLM/WizardLM-13B-V1.2\",\n",
        "    \"garage-bAInd/Platypus2-13B\",\n",
        "    # can be model+lora, like so:\n",
        "    # \"TheBloke/CodeLlama-13B-fp16+Blackroot/Llama-2-13B-Storywriter-LORA\"\n",
        "]\n",
        "\n",
        "# Tweakable parameters\n",
        "DIFF_DENSITY = 0.33  # fraction of weights from each diff vs. base model to retain\n",
        "NAIVE_SIGN_COUNT = False  # if True, will use an alternate, dumber sign consensus algorithm"
      ],
      "metadata": {
        "id": "84cRJT6_ecbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# detect device capabilities\n",
        "use_cuda = False\n",
        "int8_mask = False\n",
        "bf16 = False\n",
        "if torch.cuda.is_available():\n",
        "  use_cuda = True\n",
        "  (maj, min) = torch.cuda.get_device_capability()\n",
        "  if maj > 7 or (maj == 7 and min > 0):\n",
        "    int8_mask = True\n",
        "  if torch.cuda.is_bf16_supported():\n",
        "    bf16 = True\n",
        "\n",
        "print(f\"CUDA: {use_cuda}\")\n",
        "if use_cuda:\n",
        "  print(f\"bf16: {bf16}, int8: {int8_mask}\")"
      ],
      "metadata": {
        "id": "M92mbICKd_o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actually do merge\n",
        "%cd /content/ties-merge/\n",
        "import ties_merge\n",
        "\n",
        "ties_merge.main(\n",
        "    base_model=BASE_MODEL,\n",
        "    out_path=\"/content/merged\",\n",
        "    merge=MERGE_MODELS,\n",
        "    density=DIFF_DENSITY,\n",
        "    naive_count=NAIVE_SIGN_COUNT,\n",
        "    cuda=use_cuda,\n",
        "    int8_mask=int8_mask,\n",
        "    bf16=bf16,\n",
        "    merged_cache_dir=\"/content/tmp-lora-models/\",\n",
        ")"
      ],
      "metadata": {
        "id": "6nw26xQLkBax"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}